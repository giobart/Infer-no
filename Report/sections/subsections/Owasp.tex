\documentclass[../Report.tex]{subfiles}

\begin{document}

The OWASP benchmark project is an executable web application, provided as a Maven project, written in Java using the javax framework. It contains several Java files (slightly less than 3000): each one of this files is a Java servlet wich can contain either a true vulnerability or a false positive. As the project is provided as a public repository on GitHub, one can run both dynamic application security testing tool or static vulnerability analysis tools against it. \\
Of course, we are also provided with a csv containing the expected results for each test case, meaning that for each test we have:
\begin{itemize}
	\item The name of the test case;
	\item The vulnerability area;
	\item A boolean flag indicating if the vulnerability is a true or a false positive;
	\item The CWE number of the vulnerability.
\end{itemize}
Once the analysis is complete, we are provided with a score, called the Benchmark Accuracy Score, in the range 0-100. This score is a Youden index, computed as:
\[ J = sensitivity + specificity - 1 \]
where in turn we have:
\[ sensitivity = TP/P = TP/(TP + FN) \]
and
\[ specificity = TN/N = TN/(TN + FP) \]
Thus, we have that sensitivity represents the ability of recognizing a true positive, while the specificity is the ability of correctly identifying true negative. Looking at the formulas, it is clear that a tool which label each line of code as a vulnerability has a very low sensitivity, as the number of False Negatives is very high. At the very same way, a tool which does not recognize any vulnerability has sensitivity 0, as the TP factor is nullified. A similar consideration can be done for specificity.

\end{document}